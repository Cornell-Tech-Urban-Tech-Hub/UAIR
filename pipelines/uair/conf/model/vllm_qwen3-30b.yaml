model_source: /share/ju/matt/zoo/models/Qwen3-30B-A3B-Instruct-2507
engine_kwargs:
  enable_chunked_prefill: true
  max_num_batched_tokens: 2048
  max_model_len: 16384
  max_num_seqs: 4
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 2
  # vLLM best-practice safe defaults
  tokenizer_mode: auto            # auto-select fast/hf
  trust_remote_code: true         # allow custom model code when needed
  dtype: auto                     # let vLLM choose appropriate dtype
  kv_cache_dtype: auto            # auto-select KV cache dtype (fp8 if supported)
  enable_prefix_caching: true     # improves throughput for repeated prompts
  use_v2_block_manager: true      # newer block manager with better performance
  disable_log_stats: true         # reduce engine-side logging noise
  enforce_eager: true             # disable CUDA graphs by default for stability
batch_size: 4
concurrency: 1


