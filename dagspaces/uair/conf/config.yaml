# Top-level Hydra config for E_CI-rule-tuples

defaults:
  - _self_
  - data: inputs
  - prompt: classify
  - prompt@prompt_taxonomy: taxonomy
  - model: vllm_qwen3-30b
  - optional pipeline: cluster_topic

experiment:
  name: UAIR

runtime:
  debug: false
  sample_n: null
  output_root: null
  serialize_nested_json: true
  use_llm_classify: true
  use_llm_decompose: false
  max_errored_blocks: 0
  guided_decoding_decompose: false
  streaming_io: false
  prefilter_mode: pre_gating   # pre_gating | post_gating | off
  keyword_buffering: true
  keyword_window_words: 100
  job_memory_gb: 32
  gate_on_relevance: true
  rows_per_block: 4000

# Topic modeling configuration
topic:
  cluster_on: article       # article | chunk
  text_column: chunk_text # auto | chunk_text | article_text (default to full article)
  max_tokens_for_embed: 8192
  seed: 777
  # Optional cap to bound memory; null means unlimited
  sample_max_units: null
  # TF-IDF controls for cluster top-term summaries
  tfidf_stop_words: english   # english | null | custom list via code
  tfidf_max_df: 0.90          # ignore terms in >90% of units
  tfidf_ngram_max: 2          # up to bigrams for scalability at 386k docs
  doc_terms_k: 10             # number of per-article tf-idf keywords to emit
  embed:
    model_source: nomic-ai/nomic-embed-text-v1.5
    batch_size: 64
    device: auto            # auto | cpu | cuda
    normalize: true
    trust_remote_code: true
    matryoshka_dim: 256     # 512 for more fidelity; 256 balances speed/memory
    max_seq_length: 8192     # e.g., 8192; null = model default
    torch_dtype: float16     # float16 | bfloat16 | float32
  plot:
    method: scatter
  gpu:
    use_rapids: true         # try cuML UMAP/HDBSCAN on GPU when available
  reduce:
    method: umap            # umap | pca | none
    n_components: 15
    n_neighbors: 50
    min_dist: 0.05
    metric: cosine
  hdbscan:
    min_cluster_size: 10
    min_samples: null         # robustness at scale
    metric: euclidean
    cluster_selection_epsilon: 0.0

sampling_params:
  seed: 777
  temperature: 0.0
  top_p: 1.0
  top_k: -1
  max_tokens: 16384

# Stage-specific sampling overrides
sampling_params_classify:
  max_tokens: 4
  detokenize: false
  guided_decoding:
    choice:
      - "YES"
      - "NO"

sampling_params_decompose:
  max_tokens: 1024
  detokenize: false

# Taxonomy stage sampling overrides
sampling_params_taxonomy:
  max_tokens: 16
  detokenize: false

# Path/config for taxonomy
taxonomy_json: ${oc.env:TAXONOMY_JSON,/share/pierson/matt/UAIR/dagspaces/uair/conf/taxonomy/weitz.yaml}

# Verification defaults
verify:
  method: embed   # off | embed | nli | combo | combo_judge
  top_k: 3
  thresholds: "sim=0.55,ent=0.85,contra=0.05"
  device: ${oc.env:VERIFY_DEVICE,null}

pipeline:
  output_root: ${runtime.output_root}
  allow_partial: false
  sources:
    articles:
      path: ${data.parquet_path}
      type: parquet
  graph:
    nodes:
      topic:
        stage: topic
        depends_on: []
        inputs:
          dataset: articles
        outputs:
          docs: outputs/topic/docs_topics.parquet
        overrides:
          runtime.streaming_io: ${runtime.streaming_io}
          runtime.debug: ${runtime.debug}
          topic.embed.device: cpu
        launcher: g2_slurm_cpu
        wandb_suffix: topic

wandb:
  enabled: true
  name_prefix: null
  project: ${oc.env:WANDB_PROJECT,UAIR}
  entity: ${oc.env:WANDB_ENTITY,""}
  group: ${oc.env:WANDB_GROUP,""}
  table_sample_rows: 1000
  single_run: true


hydra:
  job:
    name: ${experiment.name}
  run:
    dir: ${oc.env:HYDRA_RUN_DIR,outputs}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${oc.env:HYDRA_SWEEP_DIR,multirun}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}



